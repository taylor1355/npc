{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPC Cognitive Pipeline Testing\n",
    "\n",
    "This notebook provides full observability into the NPC cognitive pipeline using the new LangGraph architecture with proper Godot action schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from typing import Any\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "# Import the new cognitive architecture\n",
    "from mind.cognitive_architecture.pipeline import CognitivePipeline\n",
    "from mind.cognitive_architecture.state import PipelineState\n",
    "from mind.cognitive_architecture.models import (\n",
    "    ObservationContext, \n",
    "    AvailableAction, \n",
    "    Action, \n",
    "    ActionType,\n",
    "    Memory\n",
    ")\n",
    "from mind.cognitive_architecture.memory.store import MemoryStore\n",
    "\n",
    "# Import LangChain LLM wrapper\n",
    "from mind.apis.langchain_llm import get_llm, LangChainModel\n",
    "\n",
    "# Set up logging to see all intermediate steps\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counter\n",
    "\n",
    "Simple token counting utility to measure usage at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class TokenUsage:\n",
    "    \"\"\"Track token usage for a single LLM call\"\"\"\n",
    "    step: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    duration_ms: int\n",
    "    \n",
    "class TokenTracker:\n",
    "    \"\"\"Track token usage across the pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoder = tiktoken.encoding_for_model(\"gpt-4\")  # Close enough for Claude\n",
    "        self.usage_history: list[TokenUsage] = []\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a string\"\"\"\n",
    "        return len(self.encoder.encode(text))\n",
    "    \n",
    "    def track_call(self, step: str, input_text: str, output_text: str, duration_ms: int):\n",
    "        \"\"\"Track a single LLM call\"\"\"\n",
    "        usage = TokenUsage(\n",
    "            step=step,\n",
    "            input_tokens=self.count_tokens(input_text),\n",
    "            output_tokens=self.count_tokens(output_text),\n",
    "            total_tokens=self.count_tokens(input_text) + self.count_tokens(output_text),\n",
    "            duration_ms=duration_ms\n",
    "        )\n",
    "        self.usage_history.append(usage)\n",
    "        return usage\n",
    "    \n",
    "    def summary(self) -> dict[str, Any]:\n",
    "        \"\"\"Get summary of all token usage\"\"\"\n",
    "        total_input = sum(u.input_tokens for u in self.usage_history)\n",
    "        total_output = sum(u.output_tokens for u in self.usage_history)\n",
    "        total_time = sum(u.duration_ms for u in self.usage_history)\n",
    "        \n",
    "        return {\n",
    "            \"total_input_tokens\": total_input,\n",
    "            \"total_output_tokens\": total_output,\n",
    "            \"total_tokens\": total_input + total_output,\n",
    "            \"total_duration_ms\": total_time,\n",
    "            \"steps\": [{\n",
    "                \"step\": u.step,\n",
    "                \"tokens\": u.total_tokens,\n",
    "                \"duration_ms\": u.duration_ms\n",
    "            } for u in self.usage_history]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM via OpenRouter\n",
    "# Can easily switch models using LangChainModel constants\n",
    "llm = get_llm(LangChainModel.GEMINI_FLASH_LITE)\n",
    "\n",
    "# Initialize memory store\n",
    "memory_store = MemoryStore(collection_name=\"test_memories\")\n",
    "\n",
    "# Create the cognitive pipeline\n",
    "pipeline = CognitivePipeline(llm=llm, memory_store=memory_store)\n",
    "\n",
    "print(\"Pipeline initialized with nodes:\")\n",
    "print(\"  1. Memory Query\")\n",
    "print(\"  2. Memory Retrieval\") \n",
    "print(\"  3. Cognitive Update\")\n",
    "print(\"  4. Action Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenario Setup\n",
    "\n",
    "Create a realistic test scenario using proper Godot action schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the memory store with initial memories\n",
    "initial_memories = [\n",
    "    \"Yesterday I shaped the blade and got it to the right length\",\n",
    "    \"The customer wants a ceremonial sword with intricate engravings\",\n",
    "    \"My apprentice has been learning quickly and can now help with basic tasks\",\n",
    "    \"The forge needs more coal soon, running low on supplies\",\n",
    "    \"Last week I completed a set of horseshoes for the stable\",\n",
    "    \"The village festival is coming up in three days\",\n",
    "    \"I promised my wife I would come home early today\",\n",
    "    \"The apprentice's name is Tom and he's been working with me for 3 months\",\n",
    "    \"I have a bad back from years of smithing work\",\n",
    "    \"The local inn keeper is my best customer\"\n",
    "]\n",
    "\n",
    "# Clear and repopulate memory store\n",
    "memory_store.clear()\n",
    "for memory_content in initial_memories:\n",
    "    memory = memory_store.add_memory(memory_content, importance=5.0)\n",
    "    print(f\"Added memory: {memory_content[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal memories in store: {memory_store.collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test observation context with proper Godot action schemas\n",
    "# Note: This mirrors the actual format from EventFormatter.get_available_actions() in the Godot client\n",
    "observation_context = ObservationContext(\n",
    "    agent_id=\"blacksmith_001\",\n",
    "    observation_text=\"\"\"# Current Status\n",
    "\n",
    "## Position\n",
    "You are at position (5,10).\n",
    "\n",
    "## Needs\n",
    "- Hunger: 75%\n",
    "- Energy: 60%\n",
    "- Fun: 40%\n",
    "- Hygiene: 80%\n",
    "\n",
    "## Visible Items\n",
    "\n",
    "### Tom (at position (6,10))\n",
    "Interactions:\n",
    "- **conversation**: Talk with Tom Effects: Fun (+), Energy (-)\n",
    "\n",
    "### Sword Blade (at position (5,11))\n",
    "Interactions:\n",
    "- **craft**: Continue working on the sword blade Effects: Fun (+), Energy (-)\n",
    "\n",
    "### Forge (at position (4,10))\n",
    "Interactions:\n",
    "- **restart_fire**: Restart the forge fire Effects: Energy (-)\n",
    "\n",
    "# Environment\n",
    "\n",
    "The morning sun streams through the workshop window. Your apprentice Tom just arrived and greets you cheerfully.\n",
    "The half-finished sword blade lies on your workbench. The forge fire has died down overnight.\"\"\",\n",
    "    available_actions=[\n",
    "        AvailableAction(\n",
    "            name=\"MOVE_TO\",\n",
    "            description=\"Move to a specific cell\",\n",
    "            parameters={\"destination\": \"Target position to move to\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"MOVE_DIRECTION\",\n",
    "            description=\"Start or stop continuous movement in a direction\",\n",
    "            parameters={\"direction\": \"Direction to move (e.g., (0,-1) for up)\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"INTERACT_WITH\",\n",
    "            description=\"Interact with an item or NPC\",\n",
    "            parameters={\n",
    "                \"entity_id\": \"Target entity ID\",\n",
    "                \"interaction_name\": \"Name of interaction to request\"\n",
    "            }\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"WANDER\",\n",
    "            description=\"Move to a random location\",\n",
    "            parameters={\"range\": \"Maximum wander distance from current position\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"WAIT\",\n",
    "            description=\"Wait for a specified duration\",\n",
    "            parameters={\"duration\": \"How long to wait in seconds\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"CONTINUE\",\n",
    "            description=\"Continue current activity\",\n",
    "            parameters={}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"ACT_IN_INTERACTION\",\n",
    "            description=\"Take an action within the current interaction\",\n",
    "            parameters={\n",
    "                \"message\": \"Message to send (for conversations)\",\n",
    "                \"action_type\": \"Type of action within interaction\"\n",
    "            }\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"CANCEL_INTERACTION\",\n",
    "            description=\"Stop the current interaction\",\n",
    "            parameters={}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"RESPOND_TO_INTERACTION_BID\",\n",
    "            description=\"Accept or reject an interaction request\",\n",
    "            parameters={\n",
    "                \"bid_id\": \"ID of the bid to respond to\",\n",
    "                \"accept\": \"True to accept, false to reject\",\n",
    "                \"reason\": \"Optional reason for rejection\"\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    personality_traits=[\"hardworking\", \"perfectionist\", \"patient teacher\", \"caring mentor\"]\n",
    ")\n",
    "\n",
    "# Initial working memory\n",
    "initial_working_memory = \"\"\"I am a blacksmith in the village. I've been working on a sword commission for the past two days. \n",
    "This morning I feel well-rested despite my back acting up a bit. My apprentice should help today.\n",
    "I need to finish the blade today and start on the handle tomorrow.\"\"\"\n",
    "\n",
    "print(\"Test scenario created:\")\n",
    "print(f\"  Agent: {observation_context.agent_id}\")\n",
    "print(f\"  Available actions: {len(observation_context.available_actions)}\")\n",
    "print(f\"  Personality traits: {', '.join(observation_context.personality_traits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Available Actions\n",
    "\n",
    "Show how the actions format themselves for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available Actions (as formatted for LLM):\")\n",
    "print(\"=\"*60)\n",
    "for i, action in enumerate(observation_context.available_actions, 1):\n",
    "    print(f\"{i}. {str(action)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cognitive Pipeline\n",
    "\n",
    "Execute the pipeline with full observability of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline_with_observation(pipeline, observation_context, working_memory):\n",
    "    \"\"\"Run the pipeline and display intermediate results\"\"\"\n",
    "    \n",
    "    # Create initial state\n",
    "    state = PipelineState(\n",
    "        observation_context=observation_context,\n",
    "        working_memory=working_memory\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"STARTING COGNITIVE PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nInitial State:\")\n",
    "    print(f\"  Working Memory: {len(state.working_memory)} chars\")\n",
    "    print(f\"  Observation: {len(state.observation_context.observation_text)} chars\")\n",
    "    print()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    start_time = time.time()\n",
    "    result_state = await pipeline.process(state)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Display results from each step\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE STEPS COMPLETED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n1. MEMORY QUERIES GENERATED:\")\n",
    "    print(\"-\"*40)\n",
    "    for i, query in enumerate(result_state.memory_queries, 1):\n",
    "        print(f\"  {i}. {query}\")\n",
    "    print(f\"\\nTime: {result_state.time_ms.get('memory_query', 0)}ms\")\n",
    "    \n",
    "    print(\"\\n2. MEMORIES RETRIEVED:\")\n",
    "    print(\"-\"*40)\n",
    "    for i, memory in enumerate(result_state.retrieved_memories, 1):\n",
    "        print(f\"  {i}. {memory.content}\")\n",
    "        print(f\"     (importance: {memory.importance:.1f})\")\n",
    "    print(f\"\\nTime: {result_state.time_ms.get('memory_retrieval', 0)}ms\")\n",
    "    \n",
    "    print(\"\\n3. COGNITIVE UPDATE:\")\n",
    "    print(\"-\"*40)\n",
    "    if result_state.cognitive_context:\n",
    "        print(f\"  Situation: {result_state.cognitive_context.get('situation_assessment', 'N/A')}\")\n",
    "        print(f\"  Current Goals: {', '.join(result_state.cognitive_context.get('current_goals', []))}\")\n",
    "        print(f\"  Emotional State: {result_state.cognitive_context.get('emotional_state', 'N/A')}\")\n",
    "    print(f\"\\n  Updated Working Memory Preview:\")\n",
    "    print(f\"  {result_state.working_memory[:200]}...\")\n",
    "    print(f\"\\nTime: {result_state.time_ms.get('cognitive_update', 0)}ms\")\n",
    "    \n",
    "    print(\"\\n4. ACTION SELECTED:\")\n",
    "    print(\"-\"*40)\n",
    "    if result_state.chosen_action:\n",
    "        print(f\"  Action: {result_state.chosen_action.action}\")\n",
    "        print(f\"  Parameters: {result_state.chosen_action.parameters}\")\n",
    "        print(f\"  Formatted: {str(result_state.chosen_action)}\")\n",
    "    else:\n",
    "        print(\"  No action selected\")\n",
    "    print(f\"\\nTime: {result_state.time_ms.get('action_selection', 0)}ms\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Time: {total_time:.2f} seconds\")\n",
    "    print(f\"Total Time (tracked): {sum(result_state.time_ms.values())}ms\")\n",
    "    \n",
    "    return result_state\n",
    "\n",
    "# Run the pipeline\n",
    "result = await run_pipeline_with_observation(pipeline, observation_context, initial_working_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Action for MCP Protocol\n",
    "\n",
    "Show how the selected action would be sent back to Godot via MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.chosen_action:\n",
    "    # Format as MCP response\n",
    "    mcp_response = {\n",
    "        \"action\": result.chosen_action.action,\n",
    "        \"parameters\": result.chosen_action.parameters\n",
    "    }\n",
    "    \n",
    "    print(\"MCP Response to Godot:\")\n",
    "    print(\"=\"*40)\n",
    "    print(json.dumps(mcp_response, indent=2))\n",
    "    \n",
    "    print(\"\\nThis action would cause the NPC to:\")\n",
    "    if result.chosen_action.action == \"interact_with\":\n",
    "        entity = result.chosen_action.parameters.get(\"entity_id\", \"unknown\")\n",
    "        interaction = result.chosen_action.parameters.get(\"interaction_name\", \"unknown\")\n",
    "        print(f\"  → Request a '{interaction}' interaction with entity '{entity}'\")\n",
    "        print(f\"  → Transition to NpcBiddingState in the controller\")\n",
    "    elif result.chosen_action.action == \"move_to\":\n",
    "        dest = result.chosen_action.parameters.get(\"destination\", \"unknown\")\n",
    "        print(f\"  → Navigate to grid position {dest}\")\n",
    "        print(f\"  → Transition to MovingState in the controller\")\n",
    "    elif result.chosen_action.action == \"wait\":\n",
    "        duration = result.chosen_action.parameters.get(\"duration\", 1.0)\n",
    "        print(f\"  → Pause for {duration} seconds\")\n",
    "        print(f\"  → Transition to WaitingState in the controller\")\n",
    "    elif result.chosen_action.action == \"wander\":\n",
    "        print(f\"  → Randomly explore the environment\")\n",
    "        print(f\"  → Transition to WanderingState in the controller\")\n",
    "    elif result.chosen_action.action == \"continue\":\n",
    "        print(f\"  → Continue current behavior unchanged\")\n",
    "        print(f\"  → No state transition\")\n",
    "else:\n",
    "    print(\"No action was selected by the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Scenarios\n",
    "\n",
    "Try the pipeline with different observations to see how it responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urgent scenario - low on supplies\n",
    "urgent_observation = ObservationContext(\n",
    "    agent_id=\"blacksmith_001\",\n",
    "    observation_text=\"\"\"# Current Status\n",
    "\n",
    "## Position\n",
    "You are at position (5,10).\n",
    "\n",
    "## Needs\n",
    "- Hunger: 70%\n",
    "- Energy: 55%\n",
    "- Fun: 30%\n",
    "- Hygiene: 75%\n",
    "\n",
    "## Visible Items\n",
    "\n",
    "### Tom (at position (6,10))\n",
    "Interactions:\n",
    "- **conversation**: Talk with Tom about getting more coal urgently Effects: Fun (+), Energy (-)\n",
    "\n",
    "# Environment\n",
    "\n",
    "The forge fire suddenly sputters and dies completely. You realize you're completely out of coal.\n",
    "The sword commission is due tomorrow and you need the forge to finish it. Tom looks worried.\"\"\",\n",
    "    available_actions=[\n",
    "        AvailableAction(\n",
    "            name=\"INTERACT_WITH\",\n",
    "            description=\"Interact with an item or NPC\",\n",
    "            parameters={\n",
    "                \"entity_id\": \"Target entity ID\",\n",
    "                \"interaction_name\": \"Name of interaction to request\"\n",
    "            }\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"MOVE_TO\",\n",
    "            description=\"Move to a specific cell\",\n",
    "            parameters={\"destination\": \"Target position to move to\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"WAIT\",\n",
    "            description=\"Wait for a specified duration\",\n",
    "            parameters={\"duration\": \"How long to wait in seconds\"}\n",
    "        ),\n",
    "        AvailableAction(\n",
    "            name=\"CONTINUE\",\n",
    "            description=\"Continue current activity\",\n",
    "            parameters={}\n",
    "        )\n",
    "    ],\n",
    "    personality_traits=[\"hardworking\", \"perfectionist\", \"patient teacher\", \"resourceful\"]\n",
    ")\n",
    "\n",
    "urgent_state = PipelineState(\n",
    "    observation_context=urgent_observation,\n",
    "    working_memory=result.working_memory  # Use updated working memory from previous run\n",
    ")\n",
    "\n",
    "print(\"Testing URGENT scenario:\")\n",
    "print(\"=\"*40)\n",
    "urgent_result = await pipeline.process(urgent_state)\n",
    "\n",
    "if urgent_result.chosen_action:\n",
    "    print(f\"\\nChosen Action: {urgent_result.chosen_action.action}\")\n",
    "    print(f\"Parameters: {urgent_result.chosen_action.parameters}\")\n",
    "    print(f\"\\nCognitive Context:\")\n",
    "    print(f\"  Situation: {urgent_result.cognitive_context.get('situation_assessment', 'N/A')}\")\n",
    "    print(f\"  Emotional: {urgent_result.cognitive_context.get('emotional_state', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Analysis\n",
    "\n",
    "Examine what memories were retrieved and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory Retrieval Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test memory retrieval with different queries\n",
    "test_queries = [\n",
    "    \"apprentice Tom teaching\",\n",
    "    \"sword blade commission\",\n",
    "    \"coal forge supplies\",\n",
    "    \"wife promise home\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    memories = await memory_store.search(query, top_k=2)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\"*40)\n",
    "    for i, memory in enumerate(memories, 1):\n",
    "        print(f\"  {i}. {memory.content}\")\n",
    "        print(f\"     Importance: {memory.importance:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Analyze the performance characteristics of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect timing and token data from multiple runs\n",
    "timing_results = []\n",
    "\n",
    "print(\"Running pipeline 3 times to measure performance...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(3):\n",
    "    test_state = PipelineState(\n",
    "        observation_context=observation_context,\n",
    "        working_memory=initial_working_memory\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    test_result = await pipeline.process(test_state)\n",
    "    elapsed = (time.time() - start) * 1000  # Convert to ms\n",
    "    \n",
    "    timing_results.append({\n",
    "        \"run\": i + 1,\n",
    "        \"total_ms\": elapsed,\n",
    "        \"memory_query_ms\": test_result.time_ms.get('memory_query', 0),\n",
    "        \"memory_retrieval_ms\": test_result.time_ms.get('memory_retrieval', 0),\n",
    "        \"cognitive_update_ms\": test_result.time_ms.get('cognitive_update', 0),\n",
    "        \"action_selection_ms\": test_result.time_ms.get('action_selection', 0),\n",
    "        \"memory_query_tokens\": test_result.tokens_used.get('memory_query', 0),\n",
    "        \"cognitive_update_tokens\": test_result.tokens_used.get('cognitive_update', 0),\n",
    "        \"action_selection_tokens\": test_result.tokens_used.get('action_selection', 0),\n",
    "    })\n",
    "    print(f\"Run {i+1}: {elapsed:.0f}ms total, {sum(test_result.tokens_used.values())} tokens\")\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate averages\n",
    "avg_total = sum(r['total_ms'] for r in timing_results) / len(timing_results)\n",
    "avg_query = sum(r['memory_query_ms'] for r in timing_results) / len(timing_results)\n",
    "avg_retrieval = sum(r['memory_retrieval_ms'] for r in timing_results) / len(timing_results)\n",
    "avg_cognitive = sum(r['cognitive_update_ms'] for r in timing_results) / len(timing_results)\n",
    "avg_action = sum(r['action_selection_ms'] for r in timing_results) / len(timing_results)\n",
    "\n",
    "avg_query_tokens = sum(r['memory_query_tokens'] for r in timing_results) / len(timing_results)\n",
    "avg_cognitive_tokens = sum(r['cognitive_update_tokens'] for r in timing_results) / len(timing_results)\n",
    "avg_action_tokens = sum(r['action_selection_tokens'] for r in timing_results) / len(timing_results)\n",
    "avg_total_tokens = avg_query_tokens + avg_cognitive_tokens + avg_action_tokens\n",
    "\n",
    "print(f\"Average Total Time: {avg_total:.0f}ms\")\n",
    "print(f\"\\nBreakdown by Step:\")\n",
    "print(f\"  Memory Query:     {avg_query:.0f}ms ({avg_query/avg_total*100:.1f}%) | {avg_query_tokens:.0f} tokens\")\n",
    "print(f\"  Memory Retrieval: {avg_retrieval:.0f}ms ({avg_retrieval/avg_total*100:.1f}%) | 0 tokens (vector search)\")\n",
    "print(f\"  Cognitive Update: {avg_cognitive:.0f}ms ({avg_cognitive/avg_total*100:.1f}%) | {avg_cognitive_tokens:.0f} tokens\")\n",
    "print(f\"  Action Selection: {avg_action:.0f}ms ({avg_action/avg_total*100:.1f}%) | {avg_action_tokens:.0f} tokens\")\n",
    "\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  Total: {avg_total_tokens:.0f} tokens per decision\")\n",
    "print(f\"  Cost estimate (Gemini Flash Lite): ${avg_total_tokens * 0.000000075:.6f} per decision\")\n",
    "print(f\"  Cost estimate (Claude Sonnet 4): ${avg_total_tokens * 0.000003:.6f} per decision\")\n",
    "\n",
    "print(f\"\\nObservations:\")\n",
    "llm_time = avg_query + avg_cognitive + avg_action\n",
    "print(f\"  - LLM calls account for {llm_time/avg_total*100:.1f}% of total time\")\n",
    "print(f\"  - Memory retrieval (vector search) is {avg_retrieval/avg_total*100:.1f}% of total time\")\n",
    "if avg_cognitive > avg_action:\n",
    "    print(f\"  - Cognitive update is the slowest LLM step\")\n",
    "else:\n",
    "    print(f\"  - Action selection is the slowest LLM step\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
